import re
import os
import json
import pathlib
import pickle
import pandas as pd
import numpy as np
import ipaddress
from urllib.parse import urlparse
from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

# SVD dimension after tf-idf 
N_COMPONENTS = 5
TRAIN_BLACK=22340   # index of different kind of sample
TEST_BLACK=44682
TRAIN_WHITE= 54682
TEST_WHITE=64682
# TRAIN_BLACK —— TEST_BLACK —— TRAIN_WHITE —— TEST_WHITE

# mkdir 
def mkdir_all(parent):
    part = parent.parts
    pre = ""
    for i in part:
        pre = pathlib.Path(pre).joinpath(i)
        if not pre.exists():
            pre.mkdir()


# A set of strings is transformed with a prepared tf-idf model and then downscaled with TruncatedSVD
def tfidf_transform(model_path, corpus, n_components=N_COMPONENTS):
    with open(model_path, 'rb') as f:
        tf_vectorizer = pickle.load(f)
    temp = tf_vectorizer.transform(corpus)
    size=temp.shape[-1]
    if n_components>size:
        n_components=size
    svd = TruncatedSVD(n_components=n_components,random_state=2023)
    svd.fit(temp)
    svd_result = svd.transform(temp)
    return svd_result


# Enter a string and count the percentage of numbers
def static_numeric(x):
    if len(x)==0:
        return 0
    count = 0
    for i in x:
        if i.isdigit():
            count += 1
    return count / len(x)


# Enter a string and count the percentage of English characters.
def static_character(x):
    if len(x)==0:
        return 0
    count = 0
    for i in x:
        if i.isalpha():
            count += 1
    return count / len(x)


# Enter a string and count the percentage of special characters.
def static_symbol(x):
    if len(x)==0:
        return 0
    count = 0
    for i in x:
        if not i.isidentifier():
            count += 1
    return count / len(x)


def read_file_generator(file_path):
    with open(file_path, 'r', encoding="utf-8") as f:
        while True:
            line = f.readline()
            if line:
                yield line
            else:
                break


# Determining what type a variable is
def is_which(argument):
    if not argument:
        return "invalid"
    pattern = {
        "ipv4": [r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$'],
        "ipv6": [r'^([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}$'],
        "unix_file_path": [r'^/.*'],  # 绝对路径
        "windows_file_path": [r'^"[a-zA-Z]:\\.*"',r'^[a-zA-Z]:\\.*',r'^[\w\-\u4e00-\u9fa5]+(\.[\w]+)+$'],  # 绝对路径
        "url": [r'^(?:http|ftp)s?://.*'],
        "registry_key": [r'^HKEY_.+'],
        "float": [r'^[-+]?[0-9]+\.[0-9]+$'],
        "int": [r'^[-+]?[0-9]+$'],
    }

    for key, value in pattern.items():
        for reg_mo in value:
            match = re.match(reg_mo, argument)
            if not match:
                continue
            if len(match.string)==len(argument):
                return key
    return "invalid"


# Determines the type of the variable that specifies the column column.
def row_type(x, column):
    if pd.isna(x[column]):
        return "invalid"
    return is_which(x[column])


# Returns a fill-in-the-blank value based on type
def fill_na_value(type):
    na_dict = {
        "unix_file_path": "Invalid",
        "windows_file_path": "Invalid",
        "file_name": "Invalid",
        "url": "Invalid",
        "registry_key": "Invalid",
        "ipv4": "Invalid",
        "ipv6": "Invalid",
        "float": "0.0",
        "int": "0",
        "invalid": "Invalid",
    }
    return na_dict[type]


def fill_na(x, column, na):
    if pd.isna(x[column]) or x[column + "_type"] =="invalid":
        return na
    return x[column]


class Preprocess_Argument():
    def __init__(self, data_path, output_path, tmp_path, model_path):
        self.data_path = pathlib.Path(data_path)
        self.output_path = pathlib.Path(output_path)
        self.tmp_path = pathlib.Path(tmp_path)
        self.model_path = pathlib.Path(model_path)

        self.load_data_result_path = self.tmp_path.joinpath("load_data.json")

        if not os.path.exists(self.tmp_path):
            mkdir_all(self.tmp_path)
        if not os.path.exists(self.model_path):
            mkdir_all(self.model_path)
        if not os.path.exists(self.output_path):
            mkdir_all(self.output_path)


    def load_data(self,data_path=""):
        if data_path=="":
            data_path=self.data_path
        data = []
        data_file_list = []
        for i in os.listdir(data_path):
            if ".json" in i:
                data_file_list.append(data_path.joinpath(i))

        for i in data_file_list:
            with open(i, "r", encoding="utf-8") as f:
                for line in f.readlines():
                    temp={}
                    temp['sequence']=json.loads(line)
                    if 'black' in str(i):
                        temp['label']=1
                    else:
                        temp['label']=0
                    data.append(temp)
                black_len=len(data)


        result = {}

        function_index={}
        function_index['function']={}
        function_index['length_black']=black_len
        for i_loc,i in enumerate(data):
            for j_loc,j in enumerate(i['sequence']):
                # deal with one function of sequence
                # LoadLibraryExW(C:\\WINDOWS\\system32\\shlwapi.dll)
                function_name_end = j.find("(")
                function_name = j[:function_name_end]
                raw_parameter_list = j[function_name_end + 1:-1]
                is_parameters = raw_parameter_list.find(", ")
                if is_parameters == -1:
                    if len(raw_parameter_list):
                        parameter_list = [raw_parameter_list]
                    else:
                        parameter_list = []
                else:
                    parameter_list = raw_parameter_list.split(', ')
                if function_name not in result.keys():
                    result[function_name] = []
                    function_index['function'][function_name]=[]
                temp={}
                temp['label']=i['label']
                temp['parameter']=parameter_list
                result[function_name].append(temp)

                if (i_loc>=0 and i_loc<=TRAIN_BLACK) or (i_loc>TEST_BLACK and i_loc<=TRAIN_WHITE):
                    function_index['function'][function_name].append([i_loc,j_loc,1])
                else:
                    function_index['function'][function_name].append([i_loc, j_loc,0])


        with open(self.tmp_path.joinpath("function_index.json"),'w',encoding='utf-8') as f:
            f.write(json.dumps(function_index))


        with open(self.load_data_result_path, 'w', encoding="utf-8") as f:
            for key, value in result.items():
                f.write(json.dumps([key, value], ensure_ascii=False))
                f.write("\n")
        # {'fuction_name':[{'label':0/1,'parameter':[parameter]},{'label':0/1,'parameter':[parameter]}]}


    def string_general_preprocess(self, function_name, param_name, col_type, col, new_df, sub_col, n_components):
        model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
        if not model_path.exists():
            return
        svd_result= tfidf_transform(model_path, sub_col, n_components)
        idx_len=svd_result.shape[-1]
        for i in range(idx_len):
            new_df[param_name + "_tfidf_i"] = pd.Series(svd_result[:, i])

        new_df[param_name + "_len"] = pd.Series([len(i) for i in col])
        new_df[param_name + "_numeric_percentage"] = pd.Series([static_numeric(i) for i in col])
        new_df[param_name + "_en_percentage"] = pd.Series([static_character(i) for i in col])
        new_df[param_name + "_symbol_percentage"] = pd.Series([static_symbol(i) for i in col])

    def unix_path_special_deal(self,col):
        root_ls = []
        second_ls = []
        third_ls = []
        for i in col:
            path_levels = i.split('/')
            if len(path_levels) > 1:
                root_ls.append(path_levels[1])
                if len(path_levels) > 2:
                    second_ls.append(path_levels[2])
                    if len(path_levels) > 3:
                        third_ls.append(path_levels[3])
                    else:
                        third_ls.append('')
                else:
                    second_ls.append('')
                    third_ls.append('')
            else:
                root_ls.append('')
                second_ls.append('')
                third_ls.append('')
        return root_ls,second_ls,third_ls

    def windows_path_special_deal(self,col):
        # C:\Windows\System32\aclui.dll
        root_ls = []
        second_ls = []
        third_ls = []
        for i in col:
            root_ls.append(i[0])
            path_levels = i[3:].split('\\')
            if len(path_levels) >= 1:
                second_ls.append(path_levels[0])
                if len(path_levels) >= 2:
                    third_ls.append(path_levels[1])
                else:
                    third_ls.append("")
            else:
                second_ls.append("")
                third_ls.append("")
        return root_ls,second_ls,third_ls

    def filename_special_deal(self,col):
        suffixes = []
        for i in col:
            idx = i[::-1].find('.')
            if idx != -1:
                suffixes.append(i[len(i) - idx:])
            else:
                suffixes.append('')
        return suffixes

    def url_special_deal(self,col):
        top_list = []
        second_list = []
        son_list = []
        for i in col:
            url = urlparse(i)
            host = str(url.netloc)
            host_level = host.split('.')

            if len(host_level) >= 1:
                top_list.append(host_level[-1])
                if len(host_level) >= 2:
                    second_list.append(host_level[-2])
                    if len(host_level) >= 3:
                        son_list.append(host_level[-3])
                    else:
                        son_list.append('')

                else:
                    second_list.append('')
                    son_list.append('')
            else:
                top_list.append('')
                second_list.append('')
                son_list.append('')
        return top_list,second_list,son_list

    def registry_special_deal(self,col):
        root_ls = []
        second_ls = []
        third_ls = []
        for i in col:
            path_levels = i.split('\\')
            if len(path_levels) >= 1:
                root_ls.append(path_levels[0])
                if len(path_levels) >= 2:
                    second_ls.append(path_levels[1])
                    if len(path_levels) >= 3:
                        third_ls.append(path_levels[2])
                    else:
                        third_ls.append('')
                else:
                    second_ls.append('')
                    third_ls.append('')
            else:
                root_ls.append('')
                second_ls.append('')
                third_ls.append('')
        return root_ls,second_ls,third_ls

    def preprocess(self, function_name, param_name, col_type, col, new_df):
        if col_type == "invalid":
            return
        elif col_type == 'int':
            new_col = [[float(i)] for i in col]
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            if not model_path.exists():
                return
            with open(model_path, 'rb') as f:
                scaler = pickle.load(f)
            new_df[param_name + "_standard"] = pd.Series(scaler.transform(new_col).flatten())
        elif col_type == 'float':
            new_col = [[float(i)] for i in col]
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            if not model_path.exists():
                return
            with open(model_path, 'rb') as f:
                scaler = pickle.load(f)
            new_df[param_name + "_standard"] = pd.Series(scaler.transform(new_col).flatten())
        elif col_type == 'unix_file_path':
            # TF-IDF、Len、Numeric percentage、Character percentage、symbol percentage
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace('/', ' '))

            n_components = N_COMPONENTS
            self.string_general_preprocess(function_name, param_name, col_type, col, new_df, sub_col, n_components)

            root_ls,second_ls,third_ls=self.unix_path_special_deal(col)
            special = {'root': root_ls, 'second': second_ls, 'third': third_ls}

            for key, value in special.items():
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                if not model_path.exists():
                    continue
                with open(model_path, 'rb') as f:
                    encoder=pickle.load(f)
                    new_value=[[i_value] for i_value in value]
                    y=encoder.transform(new_value)
                    y=y.flatten()
                    new_df[param_name+"_"+key]=pd.Series(y)

        elif col_type == 'windows_file_path':
            new_col = []
            for i in col:
                if i.find('"')==-1:
                    new_col.append(i.replace('"',''))
                else:
                    new_col.append(i)
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace(':\\', ' ').replace('\\', ' '))

            n_components = N_COMPONENTS
            self.string_general_preprocess(function_name, param_name, col_type, col, new_df, sub_col, n_components)

            root_ls,second_ls,third_ls=self.windows_path_special_deal(col)
            special = {'root': root_ls, 'second': second_ls, 'third': third_ls}

            for key, value in special.items():
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                if not model_path.exists():
                    continue
                with open(model_path, 'rb') as f:
                    encoder = pickle.load(f)
                    new_value = [[i_value] for i_value in value]
                    y = encoder.transform(new_value)
                    y = y.flatten()
                    new_df[param_name + "_" + key] = pd.Series(y)

        elif col_type == "file_name":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace('.', ' '))

            n_components = N_COMPONENTS
            self.string_general_preprocess(function_name, param_name, col_type, col, new_df, sub_col, n_components)

            suffixes = self.filename_special_deal(col)
            special = {'suffix': suffixes}

            for key, value in special.items():
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                if not model_path.exists():
                    continue
                with open(model_path, 'rb') as f:
                    encoder = pickle.load(f)
                    new_value = [[i_value] for i_value in value]
                    y = encoder.transform(new_value)
                    y = y.flatten()
                    new_df[param_name + "_" + key] = pd.Series(y)

        elif col_type == "url":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                temp = single.replace('://', ' ')
                if temp.find('/') != -1:
                    temp = temp[:temp.find('/')].replace('.', ' ')
                else:
                    temp = temp.replace('.', ' ')
                sub_col.append(temp)

            n_components = N_COMPONENTS
            self.string_general_preprocess(function_name, param_name, col_type, col, new_df, sub_col, n_components)

            top_list,second_list,son_list=self.url_special_deal(col)
            special = {'top': top_list, 'second': second_list, 'third': son_list}

            for key, value in special.items():
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                if not model_path.exists():
                    continue
                with open(model_path, 'rb') as f:
                    encoder = pickle.load(f)
                    new_value = [[i_value] for i_value in value]
                    y = encoder.transform(new_value)
                    y = y.flatten()
                    new_df[param_name + "_" + key] = pd.Series(y)

        elif col_type == "registry_key":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace('/', ' '))

            n_components = N_COMPONENTS
            self.string_general_preprocess(function_name, param_name, col_type, col, new_df, sub_col, n_components)

            root_ls,second_ls,third_ls=self.registry_special_deal(col)
            special = {'root': root_ls, 'second': second_ls, 'third': third_ls}

            for key, value in special.items():
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                if not model_path.exists():
                    continue
                with open(model_path, 'rb') as f:
                    encoder = pickle.load(f)
                    new_value = [[i_value] for i_value in value]
                    y = encoder.transform(new_value)
                    y = y.flatten()
                    new_df[param_name + "_" + key] = pd.Series(y)

        elif col_type == "ipv4":
            new_col = [i for i in col]
            is_private = []
            for i in new_col:
                if ipaddress.ip_address(i).is_private:
                    is_private.append(1)
                else:
                    is_private.append(0)
            new_df[param_name+"_is_private"]=pd.Series(is_private)

        elif col_type == "ipv6":
            new_col = [i for i in col]
            is_private = []
            for i in new_col:
                if ipaddress.ip_address(i).is_private:
                    is_private.append(1)
                else:
                    is_private.append(0)
            new_df[param_name + "_is_private"] = pd.Series(is_private)

    def preprocess_init(self, function_name, param_name, col_type, col):
        # Model initialization for preprocessing
        if col_type == "invalid":
            return
        elif col_type == "int":
            new_col = [[float(i)] for i in col]
            scaler = StandardScaler()
            scaler.fit(new_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(scaler, f)

        elif col_type == 'float':
            new_col = [[float(i)] for i in col]
            scaler = StandardScaler()
            scaler.fit(new_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(scaler, f)

        elif col_type == 'unix_file_path':
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace('/', ' '))
            tf_vectorizer = TfidfVectorizer()
            tf_vectorizer.fit(sub_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(tf_vectorizer, f)


            root_ls, second_ls, third_ls = self.unix_path_special_deal(col)
            special={'root':root_ls,'second':second_ls,'third':third_ls}

            for key,value in special.items():
                new_value=[[i_value] for i_value in value]
                encoder=OrdinalEncoder(handle_unknown="use_encoded_value",unknown_value=-1)
                encoder.fit(new_value)
                model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                with open(model_path, 'wb') as f:
                    pickle.dump(encoder, f)


        elif col_type == "windows_file_path":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace(':\\', ' ').replace('\\', ' '))
            tf_vectorizer = TfidfVectorizer()
            tf_vectorizer.fit(sub_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(tf_vectorizer, f)

            root_ls, second_ls, third_ls = self.windows_path_special_deal(col)
            special = {'root': root_ls, 'second': second_ls, 'third': third_ls}

            for key, value in special.items():
                new_value = [[i_value] for i_value in value]
                encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
                encoder.fit(new_value)
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                with open(model_path, 'wb') as f:
                    pickle.dump(encoder, f)

        elif col_type == "file_name":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace('.', ' '))
            tf_vectorizer = TfidfVectorizer()
            tf_vectorizer.fit(sub_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(tf_vectorizer, f)

            suffixes = self.filename_special_deal(col)
            special = {'suffix': suffixes}

            for key, value in special.items():
                new_value = [[i_value] for i_value in value]
                encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
                encoder.fit(new_value)
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                with open(model_path, 'wb') as f:
                    pickle.dump(encoder, f)

        elif col_type == "url":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                temp = single.replace('://', ' ')
                if temp.find('/') != -1:
                    temp = temp[:temp.find('/')].replace('.', ' ')
                else:
                    temp = temp.replace('.', ' ')
                sub_col.append(temp)
            tf_vectorizer = TfidfVectorizer()
            tf_vectorizer.fit(sub_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(tf_vectorizer, f)

            top_list, second_list, son_list = self.url_special_deal(col)
            special = {'top': top_list, 'second': second_list, 'third': son_list}

            for key, value in special.items():
                new_value = [[i_value] for i_value in value]
                encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
                encoder.fit(new_value)
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                with open(model_path, 'wb') as f:
                    pickle.dump(encoder, f)

        elif col_type == "registry_key":
            new_col = [i for i in col]
            sub_col = []
            for single in new_col:
                sub_col.append(single.replace('/', ' '))
            tf_vectorizer = TfidfVectorizer()
            tf_vectorizer.fit(sub_col)
            model_path = self.model_path.joinpath(function_name + "__" + param_name + "__" + col_type + ".pickle")
            with open(model_path, 'wb') as f:
                pickle.dump(tf_vectorizer, f)

            root_ls, second_ls, third_ls = self.registry_special_deal(col)
            special = {'root': root_ls, 'second': second_ls, 'third': third_ls}

            for key, value in special.items():
                new_value = [[i_value] for i_value in value]
                encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
                encoder.fit(new_value)
                model_path = self.model_path.joinpath(
                    function_name + "__" + param_name + "__" + col_type + '__' + key + ".pickle")
                with open(model_path, 'wb') as f:
                    pickle.dump(encoder, f)

        elif col_type == "ipv4":
            pass

        elif col_type == "ipv6":
            pass

    def general_process(self, param_name,col,new_df):
        new_df["general_"+param_name + "_len"] = pd.Series([len(i) for i in col])
        new_df["general_"+param_name + "_numeric_percentage"] = pd.Series([static_numeric(i) for i in col])
        new_df["general_"+param_name + "_en_percentage"] = pd.Series([static_character(i) for i in col])
        new_df["general_"+param_name + "_symbol_percentage"] = pd.Series([static_symbol(i) for i in col])

    def train(self):
        self.load_data()
        print("loaded data")

        function_col_type={}

        rr=-1
        for line in read_file_generator(self.load_data_result_path):
            rr+=1

            key, value = json.loads(line)
            print(str(rr)+" "+"Currently being processed: "+key)
            labels=[i['label'] for i in value]
            parameter=[i['parameter'] for i in value]
            max_param_len = max([len(i) for i in parameter])
            if not max_param_len:
                continue
            df_columns = ["param_" + str(i + 1) for i in range(max_param_len)]
            df = pd.DataFrame(parameter, columns=df_columns)
            df['label']=pd.Series(labels)
            new_df = pd.DataFrame()
            function_col_type[key]=[]
            for i in df_columns:
                # all variable elements need to be set to the same type
                df[i + "_type"] = df.apply(lambda row: row_type(row, i), axis=1, result_type="expand")
                col_type = df[i + "_type"].mode().values[0]
                function_col_type[key].append(col_type)
                self.general_process(i,df[i].tolist(),new_df)
                na = fill_na_value(col_type)
                df[i + "_na"] = df.apply(lambda row: fill_na(row, i, na), axis=1, result_type="expand")

                # preprocess
                self.preprocess_init(key, i, col_type, df[i + "_na"].tolist())
                self.preprocess(key, i, col_type, df[i + "_na"].tolist(), new_df)

            if not new_df.empty:
                # Export intermediate results
                concat_df=pd.concat([df,new_df],axis=1)
                unique_df=concat_df.drop_duplicates()
                df_dict=unique_df.to_dict()
                with open(self.output_path.joinpath("map_"+key+".json"),'w',encoding='utf-8') as f:
                    f.write(json.dumps(df_dict,ensure_ascii=False))

                new_df['label']=pd.Series(labels)
                new_df_dict=new_df.to_dict()
                with open(self.output_path.joinpath(key+".json"),'w',encoding='utf-8') as f:
                    f.write(json.dumps(new_df_dict,ensure_ascii=False))

        print("have preprocessed")
        with open(self.tmp_path.joinpath("function_col_type.json"),'w',encoding='utf-8') as f:
            f.write(json.dumps(function_col_type,ensure_ascii=False))

    # 针对某个函数,返回其预处理结果
    # 函数名，预处理结果数组
    def test(self, function_str):
        with open(self.tmp_path.joinpath("function_col_type.json"),'r',encoding='utf-8') as f:
            function_col_type=json.loads(f.read())

        # LoadLibraryExW(C:\\WINDOWS\\system32\\shlwapi.dll)
        j=function_str
        function_name_end = j.find("(")
        function_name = j[:function_name_end]
        raw_parameter_list = j[function_name_end + 1:-1]
        is_parameters = raw_parameter_list.find(", ")
        if is_parameters == -1:
            # 这里包含了无参数和只有一个参数的情况
            if len(raw_parameter_list):
                parameter_list = [raw_parameter_list]
            else:
                parameter_list = []
        else:
            parameter_list = raw_parameter_list.split(', ')

        parameter = parameter_list

        max_param_len=len(parameter)
        if not max_param_len:
            return function_name,np.array([])
        df_columns = ["param_" + str(i + 1) for i in range(max_param_len)]
        df = pd.DataFrame([parameter], columns=df_columns)
        new_df=pd.DataFrame()
        for idx,i in enumerate(df_columns):
            df[i + "_type"] = df.apply(lambda row: row_type(row, i), axis=1, result_type="expand")
            col_type=function_col_type[function_name][idx]
            na = fill_na_value(col_type)
            df[i + "_na"] = df.apply(lambda row: fill_na(row, i, na), axis=1, result_type="expand")

            self.preprocess(function_name, i, col_type, df[i + "_na"].tolist(), new_df)

        if not new_df.empty:
            return function_name, new_df.values.tolist()
        else:
            return function_name, []


    # 得把这个函数改成对某个函数的文件（CreateProcessInternalW.json）进行读取然后返回array和label
    def read_output(self,path):
        with open(path, 'r', encoding='utf-8') as f:
            output_df=pd.read_json(f.read())

        return output_df


if __name__ == "__main__":
    p = Preprocess_Argument("../data/", "preprocess_output_CLUSTER_3/", "tmp_CLUSTER_3/", "preprocess_model_CLUSTER_3/")
    # p.train()
    print(p.test("Fake_BeCreatedEx(SyStem.exe, 212, \"C:\\program\\1.exe\" , direct)"))

