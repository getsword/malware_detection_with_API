import pathlib
import json
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import torch.utils.data as Data
from Out_Embedding import Out_Embedding
from torchtext import data
from torchtext.vocab import Vectors
from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score
import time
from tqdm import tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


'''
预处理
raw_database：
{'text':'api_label', 'label':'1'}
{'text':'api_label', 'label':'1'}
'''


vector_path="../../word2vec/vector/native_vector_embed_unlabel.txt"
train_path="../DNN_data/unlabel_train.json"
test_path="../DNN_data/unlabel_test.json"
name="unlabel"

# 定义Field
def tokenizer(text):
    return text.split(' ')

stop_words = []  # 加载停用词表
text = data.Field(sequential=True,
                  fix_length=1000,
                  tokenize=tokenizer,
                  stop_words=stop_words)
label = data.Field(sequential=False)

# 定义Database
train, test= data.TabularDataset.splits(
    path='../DNN_data/',
    skip_header=True,
    train=train_path,
    validation=test_path,
    format='json',
    fields={'text':('text',text),'label':('label',label)},
)
# 'name':('n',NAME),
# 上面这个field字典的意思就是json的每个属性应该用哪种Field处理，并且处理之后新的属性名是什么。eg.‘name’应该被NAME处理，且处理之后它将被重命名为’n’.后续构建iterator之后就可以用batch.n来访问

# 建立vocab

text.build_vocab(train, test, vectors=Vectors(name=vector_path))
label.build_vocab(train, test)

embedding_dim = text.vocab.vectors.size()[-1]
vectors = text.vocab.vectors

# 构建迭代器
batch_size=128
train_iter,test_iter = data.Iterator.splits(
            (train,test),
            sort_key=lambda x: len(x.text),
            batch_sizes=(batch_size,batch_size), # 训练集设置batch_size,验证集整个集合用于测试
            device=device
    )

vocab_size = len(text.vocab)
label_num = len(label.vocab)


'''
训练
'''


def training(train_iter, test_iter, net, num_epochs):
    batch_count = 0
    val_iter=[]
    output_path=pathlib.Path("../output_embedding/")
    for epoch in range(num_epochs):
        train_X,train_y=[],[]
        for batch_idx, batch in tqdm(enumerate(train_iter),total=len(train_iter),leave=True):
            X, y = batch.text, batch.label
           # X = X.permute(1, 0)
            y_hat = net(X)
            y.data.sub_(1)
            train_X+=y_hat.cpu().tolist()
            train_y+=y.cpu().tolist()
            # 1000,128,50
            # sequence_len,batch_size,embedding_size

        train_X = np.array(train_X)
        train_y = np.array(train_y)
        print(train_X.shape)
        print(train_y.shape)
        np.savez("../DNN_data/vector_" + name + "_train", train_X, train_y)

        train_X,train_y=[],[]

        test_X,test_y=[],[]
        for batch_idx,batch in tqdm(enumerate(test_iter),total=len(test_iter),leave=True):
            X, y = batch.text, batch.label
            # X = X.permute(1, 0)
            y_hat = net(X)
            y.data.sub_(1)
            test_X += y_hat.cpu().tolist()
            test_y += y.cpu().tolist()



        test_X=np.array(test_X)
        test_y=np.array(test_y)
        print(test_X.shape)
        print(test_y.shape)
        np.savez("../DNN_data/vector_"+name+"_test",test_X,test_y)


EPOCH=1
lr=0.001

model = Out_Embedding(vocab_size,embedding_dim,vectors).to(device)
print(model)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=lr)


training(train_iter, test_iter, model, EPOCH)

