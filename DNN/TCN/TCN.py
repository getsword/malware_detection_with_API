from torch import nn
import torch
from torch.nn.utils import weight_norm

class Crop(nn.Module):
    def __init__(self, crop_size):
        super(Crop, self).__init__()
        self.crop_size = crop_size

    def forward(self, x):
        return x[:, :, :-self.crop_size].contiguous()


class TemporalCasualLayer(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, dropout=0.2):
        super(TemporalCasualLayer, self).__init__()
        padding = (kernel_size - 1) * dilation
        conv_params = {
            'kernel_size': kernel_size,
            'stride': stride,
            'padding': padding,
            'dilation': dilation
        }

        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, **conv_params))
        self.crop1 = Crop(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, **conv_params))
        self.crop2 = Crop(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        self.net = nn.Sequential(self.conv1, self.crop1, self.relu1, self.dropout1,
                                 self.conv2, self.crop2, self.relu2, self.dropout2)
        # shortcut connect
        self.bias = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()

    def forward(self, x):
        y = self.net(x)
        b = x if self.bias is None else self.bias(x)
        return self.relu(y + b)


class TemporalConvolutionNetwork(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
        super(TemporalConvolutionNetwork, self).__init__()
        layers = []
        num_levels = len(num_channels)
        tcl_param = {
            'kernel_size': kernel_size,
            'stride': 1,
            'dropout': dropout
        }
        for i in range(num_levels):
            dilation = 2 ** i
            in_ch = num_inputs if i == 0 else num_channels[i - 1]
            out_ch = num_channels[i]
            tcl_param['dilation'] = dilation
            tcl = TemporalCasualLayer(in_ch, out_ch, **tcl_param)
            layers.append(tcl)

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class TCN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, vectors, num_inputs=100, num_channels=[200,200,200,200], class_num=2, kernel_size=2, dropout=0.2):
        super(TCN,self).__init__()
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.word_embeddings = self.word_embeddings.from_pretrained(
            vectors, freeze=True)

        self.TCN=TemporalConvolutionNetwork(num_inputs,num_channels,kernel_size,dropout)
        self.linear=nn.Linear(num_channels[-1],class_num)

    def forward(self,x):
        embeddings=self.word_embeddings(x)
        # [seq_len,batch_size,embedding_dim]
        embeddings=torch.transpose(embeddings,0,1)
        embeddings=torch.transpose(embeddings,1,2)
        # [batch_size, input_channel, seq_len]
        y=self.TCN(embeddings)
        # [batch_size, num_channel[-1], seq_len]
        y = torch.mean(y, dim=2)
        return self.linear(y)


class TCN_LSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, vectors, num_inputs=100, num_channels=[200,200,200,200], class_num=2, kernel_size=2, dropout=0.2, hidden_size=200, num_layers=2, batch_first=True, bidirectional= False):
        super(TCN_LSTM,self).__init__()
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.word_embeddings = self.word_embeddings.from_pretrained(
            vectors, freeze=True)

        self.TCN=TemporalConvolutionNetwork(num_inputs,num_channels,kernel_size,dropout)
        self.LSTM=nn.LSTM(input_size= num_channels[-1], hidden_size=hidden_size,num_layers=num_layers,batch_first=batch_first,bidirectional=bidirectional)
        linear_input=hidden_size
        if bidirectional:
            linear_input=hidden_size*2
        self.linear=nn.Linear(linear_input,class_num)

    def forward(self,x):
        embeddings=self.word_embeddings(x)
        # [seq_len,batch_size,embedding_dim]
        embeddings=torch.transpose(embeddings,0,1)
        embeddings=torch.transpose(embeddings,1,2)
        # [batch_size, input_channel, seq_len]
        y=self.TCN(embeddings)
        # [batch_size, out_channel[-1], seq_len]
        y=torch.transpose(y,1,2)
        y, (h_n, c_n) = self.LSTM(y)
        y=torch.mean(y,dim=1)
        return self.linear(y)


class TCN_GRU(nn.Module):
    def __init__(self, vocab_size, embedding_dim, vectors, num_inputs=100, num_channels=[200, 200, 200, 200],
                 class_num=2, kernel_size=2, dropout=0.2, hidden_size=200, num_layers=2, batch_first=True,
                 bidirectional=False):
        super(TCN_GRU, self).__init__()
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.word_embeddings = self.word_embeddings.from_pretrained(
            vectors, freeze=True)

        self.TCN = TemporalConvolutionNetwork(num_inputs, num_channels, kernel_size, dropout)
        self.GRU = nn.GRU(input_size= num_channels[-1], hidden_size=hidden_size, num_layers=num_layers, batch_first=batch_first, bidirectional=bidirectional)
        linear_input = hidden_size
        if bidirectional:
            linear_input = hidden_size * 2
        self.linear = nn.Linear(linear_input, class_num)

    def forward(self, x):
        embeddings = self.word_embeddings(x)
        # [seq_len,batch_size,embedding_dim]
        embeddings = torch.transpose(embeddings, 0, 1)
        embeddings = torch.transpose(embeddings, 1, 2)
        # [batch_size, input_channel, seq_len]
        y = self.TCN(embeddings)
        # [batch_size, out_channel[-1], seq_len]
        y = torch.transpose(y, 1, 2)
        y, _= self.GRU(y)
        # y=torch.mean(y,dim=1)
        y=y[:,-1,:]
        return self.linear(y)

