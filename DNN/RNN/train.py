import torch
import random
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import torch.utils.data as Data
from RNN import RNN
from torchtext import data
from torchtext.vocab import Vectors
from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score
import time

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

SEED=2023
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

vector_path="../../word2vec/vector/native_vector_embed_unlabel.txt"
train_path="../DNN_data/unlabel_train.json"
test_path="../DNN_data/unlabel_test.json"

batch_size=32

'''
raw_database：
{'text':'api_label', 'label':'1'}
{'text':'api_label', 'label':'1'}
'''

def tokenizer(text):
    return text.split(' ')

stop_words = [] 
text = data.Field(sequential=True,
                  fix_length=1000,
                  tokenize=tokenizer,
                  stop_words=stop_words)
label = data.Field(sequential=False)

# Database
train, test= data.TabularDataset.splits(
    path='../DNN_data/',
    skip_header=True,
    train=train_path,
    validation=test_path,
    #train='unlabel_train.json',
    #validation='unlabel_test.json',
    format='json',
    fields={'text':('text',text),'label':('label',label)},
)

text.build_vocab(train, test, vectors=Vectors(name=vector_path))
label.build_vocab(train, test)

embedding_dim = text.vocab.vectors.size()[-1]
vectors = text.vocab.vectors

# iterator
train_iter,test_iter = data.Iterator.splits(
            (train,test),
            sort_key=lambda x: len(x.text),
            batch_sizes=(batch_size,batch_size), 
            device=device
    )

vocab_size = len(text.vocab)
label_num = len(label.vocab)


'''
training
'''
# accuracy_score,precision_score,f1_score,recall_score
def evaluate_accuracy(data_iter, net):
    predict=[]
    label=[]
    n=0
    with torch.no_grad():
        for batch_idx, batch in enumerate(data_iter):
            X, y = batch.text, batch.label
            y.data.sub_(1)
            label+=y.cpu().detach().tolist()
            if isinstance(net, torch.nn.Module):
                net.eval() 
                predict+=net(X).argmax(dim=1).cpu().detach().tolist()
                net.train() 
            else:
                if('is_training' in net.__code__.co_varnames):
                    # 
                    predict += net(X,is_training=False).argmax(dim=1).cpu().detach().tolist()
                else:
                    predict += net(X).argmax(dim=1).cpu().detach().tolist()
            n += y.shape[0]
    print(n)
    return accuracy_score(label,predict),precision_score(label,predict,average="macro"),f1_score(label,predict,average="macro"),recall_score(label,predict,average="macro")


def training(train_iter, test_iter, net, loss, optimizer, num_epochs,max_f1):
    batch_count = 0
    val_iter=[]
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()
        for batch_idx, batch in enumerate(train_iter):
            if batch_idx in val_batch:
                val_iter.append(batch)
                continue
            X, y = batch.text, batch.label
           # X = X.permute(1, 0)
            y.data.sub_(1)
            y_hat = net(X)
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
            n += y.shape[0]
            batch_count += 1

        accuracy, precision, f1, recall = evaluate_accuracy(val_iter, net)
        print(
            "loss： %.4f, train acc： %.5f, val acc： %.5f, val precision： %.5f, val f1_score： %.5f, val recall： %.5f, time： %.1f sec"
            % (train_l_sum / batch_count, train_acc_sum / n,
               accuracy, precision, f1, recall, time.time() - start))

        accuracy, precision, f1, recall= evaluate_accuracy(test_iter, net)
        if f1>max_f1:
            max_f1=f1
            torch.save(model,"Bi-LSTM.pth")
        print(
             "test acc： %.5f, test precision： %.5f, f1_score： %.5f, recall： %.5f"
            % ( accuracy, precision, f1, recall))
    return max_f1


EPOCH=1
n_hidden = 64  # number of hidden units in one cell
n_layer=3
num_classes = label_num
lr=0.001

model = RNN(vocab_size,embedding_dim,n_hidden,n_layer,num_classes,vectors).to(device)
print(model)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=lr)


# K-fold cross-validation
K=5
max_f1=0
for i in range(K):
    print('第 '+str(i+1)+" fold:")
    fold_len=len(train_iter)/K
    val_batch=range(int(fold_len*i),int(fold_len*(i+1)))
    max_f1=training(train_iter, test_iter, model, criterion, optimizer, EPOCH, max_f1)
