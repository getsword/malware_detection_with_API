import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import os
from torch.nn import init
from torchtext import data
from torchtext.vocab import Vectors
import time
from TextCNN import TextCNN
from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

SEED=2023
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

'''
raw_database：
{'text':'api_label', 'label':'1'}
{'text':'api_label', 'label':'1'}
'''

'''
vector_path="../../word2vec/vector/native_vector_embed_3.txt"
train_path="../DNN_data/train_CLUSTER_3.json"
test_path="../DNN_data/test_CLUSTER_3.json"
'''
vector_path="../../word2vec/vector/native_vector_embed_unlabel.txt"
train_path="../DNN_data/unlabel_train.json"
test_path="../DNN_data/unlabel_test.json"

batch_size=128


def tokenizer(text):
    return text.split(' ')

stop_words = []  # 加载停用词表
text = data.Field(sequential=True,
                  tokenize=tokenizer,
                  fix_length=1000,
                  stop_words=stop_words)
label = data.Field(sequential=False)

# load Database
train, val= data.TabularDataset.splits(
    path='../DNN_data/',
    skip_header=True,
    train=train_path,
    validation=test_path,
    format='json',
    fields={'text':('text',text),'label':('label',label)},
)

# build vocab
text.build_vocab(train, val, vectors=Vectors(name=vector_path))
label.build_vocab(train, val)

embedding_dim = text.vocab.vectors.size()[-1]
vectors = text.vocab.vectors

# iterator

train_iter, test_iter = data.Iterator.splits(
            (train, val),
            sort_key=lambda x: len(x.text),
            batch_sizes=(batch_size, batch_size), 
            device=device
    )
vocab_size = len(text.vocab)
label_num = len(label.vocab)


'''
training
'''

kernel_sizes, num_channels =[3,], [100,]
net = TextCNN(vocab_size, embedding_dim, kernel_sizes, num_channels,vectors).to(device)
print(net)


def training(train_iter, test_iter, net, loss, optimizer, num_epochs,max_f1):
    batch_count = 0
    val_iter=[]
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()
        for batch_idx, batch in enumerate(train_iter):
            if batch_idx in val_batch:
                val_iter.append(batch)
                continue
            X, y = batch.text, batch.label
            X = X.permute(1, 0)
            y.data.sub_(1)  
            y_hat = net(X)
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
            n += y.shape[0]
            batch_count += 1


        accuracy, precision, f1, recall = evaluate_accuracy(val_iter, net)
        print(
            "loss： %.4f, train acc： %.5f, val acc： %.5f, val precision： %.5f, val f1_score： %.5f, val recall： %.5f, time： %.1f sec"
            % (train_l_sum / batch_count, train_acc_sum / n,
               accuracy, precision, f1, recall, time.time() - start))

        accuracy, precision, f1, recall = evaluate_accuracy(test_iter, net)
        if f1 > max_f1:
            max_f1 = f1
            torch.save(net, "Bi-LSTM.pth")
        print(
            "test acc： %.5f, test precision： %.5f, f1_score： %.5f, recall： %.5f"
            % (accuracy, precision, f1, recall))
    return max_f1

# accuracy_score,precision_score,f1_score,recall_score
def evaluate_accuracy(data_iter, net):
    predict=[]
    label=[]
    n=0
    with torch.no_grad():
        for batch_idx, batch in enumerate(data_iter):
            X, y = batch.text, batch.label
            X = X.permute(1, 0)
            y.data.sub_(1)  
            label+=y.cpu().detach().tolist()
            if isinstance(net, torch.nn.Module):
                net.eval() 
                predict+=net(X).argmax(dim=1).cpu().detach().tolist()
                net.train() 
            else: 
                if('is_training' in net.__code__.co_varnames): 
                    predict += net(X,is_training=False).argmax(dim=1).cpu().detach().tolist()
                else:
                    predict += net(X).argmax(dim=1).cpu().detach().tolist()
            n += y.shape[0]
    return accuracy_score(label,predict),precision_score(label,predict,average="macro"),f1_score(label,predict,average="macro"),recall_score(label,predict,average="macro")

lr, EPOCH = 0.001, 1
optimizer = torch.optim.Adam(net.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss().to(device)


# K-fold cross-validation
K=5
max_f1=0
for i in range(K):
    print('第 '+str(i+1)+" fold:")
    fold_len=len(train_iter)/K
    val_batch=range(int(fold_len*i),int(fold_len*(i+1)))
    max_f1=training(train_iter, test_iter, net, criterion, optimizer, EPOCH, max_f1)



