# TextCNN与用在图像领域的CNN的不同：
（1）图像是二维数据, 图像的卷积核是从左到右, 从上到下进行滑动来进行特征抽取。

（2）自然语言是一维数据, 虽然经过word-embedding 生成了二维向量，但是对词向量做从左到右滑动来进行卷积没有意义. 比如 "今天" 对应的向量[0, 0, 0, 0, 1], 按窗口大小为 1* 2 从左到右滑动得到[0,0], [0,0], [0,0], [0, 1]这四个向量, 对应的都是"今天"这个词汇, 这种滑动没有帮助.

# TextCNN
textCNN模型主要使用了一维卷积层和时序最大池化层。

一维卷积层相当于高为1的二维卷积层，多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果。

![img_1.png](img_1.png)

时序最大池化（max-over-time pooling）层实际上对应一维全局最大池化层，特点是卷积窗口和输入数组的宽高对应相同，每个通道只输出一个元素。

假设输入的文本序列由n个词组成，每个词用d维的词向量表示。那么输入样本的宽为n，高为1，输入通道数为d。卷积核的高度为d。卷积核的个数即为输出通道数。textCNN的计算主要分为以下几步：

1.定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。

2.对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。

3.通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。


![img.png](img.png)

上图用一个例子解释了textCNN的设计。这里的输入是一个有11个词的句子，每个词用6维词向量表示。因此输入序列的宽为11，输入通道数为6。给定2个一维卷积核，核宽分别为2和4，输出通道数分别设为4和5。因此，一维卷积计算后，4个输出通道的宽为11−2+1=10，而其他5个通道的宽为11−4+1=8。尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将9个通道的池化输出连结成一个9维向量。最终，使用全连接将9维向量变换为2维输出，即正面情感和负面情感的预测（概率）。



