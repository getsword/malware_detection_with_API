import torch
from torch import nn
import collections
import sys

DATASET_PATH=sys.argv[1]
LOAD_PATH=sys.argv[2]
DUMP_PATH=sys.argv[3]


def main():
    with open(DATASET_PATH, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        raw_dataset = [i.split() for i in lines]

    '''
    for st in raw_dataset[:5]:
        print("# token",len(st),st[:5])
    '''

    counter = collections.Counter([tk for st in raw_dataset for tk in st])  
    counter = dict(filter(lambda x: x[1] >= 5, counter.items()))  

    idx_to_token = [tk for tk, _ in counter.items()]
    token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}

    net=torch.load(LOAD_PATH)

    word_vector=net[0].weight

    with open(DUMP_PATH,'w',encoding='utf-8') as f:
        for key,value in token_to_idx.items():
            f.write(key+" "+str(' '.join([str(i) for i in word_vector[value].tolist()])))
            f.write('\n')


if __name__=="__main__":
    main()